{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8635d36b",
   "metadata": {},
   "source": [
    "**What is loss function**\n",
    "_______________________________\n",
    "Loss function is a method of evluating how well the algorithm is modelling the dataset.\n",
    "_____________________\n",
    "- Loss function is way to measure the algorithm\n",
    "_______________________\n",
    "**Loss function in Deep Learning**\n",
    "_____________________________\n",
    "- We start with random values of thd weights and bias in the each neurons.\n",
    "- Then we do the forward propagation and we get the ouput.\n",
    "- Then we use the output to find the loss.\n",
    "- Then by use of the loss and the gradient descent we change the values of wights and the bias through back  propagation and in the next epoch we apply the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74b0f20",
   "metadata": {},
   "source": [
    "## Loss Functions in Deep Learning\n",
    "\n",
    "### Regression\n",
    "- **MSE** (Mean Squared Error)\n",
    "- **MAE** (Mean Absolute Error)\n",
    "- **Huber Loss**\n",
    "\n",
    "### Classification\n",
    "- **Binary Crossentropy**\n",
    "- **Categorical Crossentropy**\n",
    "- **Hinge Loss**\n",
    "\n",
    "### Autoencoders\n",
    "- **KL Divergence**\n",
    "\n",
    "### GAN (Generative Adversarial Networks)\n",
    "- **Discriminator Loss**\n",
    "- **Minimax GAN Loss**\n",
    "\n",
    "### Embedding\n",
    "- **Triplet Loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac16ba3",
   "metadata": {},
   "source": [
    "**Cost function**\n",
    "- when we find the loss of the whole batch we call it the cost function\n",
    "_______________________________\n",
    "1. **Mean squared error(MSE)**\n",
    "    - for the regression loss.\n",
    "    - $(y_i-\\hat{y_i})^2$\n",
    "    - Adv\n",
    "        - East to interpret\n",
    "        - Diffrentiable\n",
    "        - 1 local minimum\n",
    "    - Dis adv\n",
    "        - error unit\n",
    "        - robust to outliers(update will be done to handle outliers)\n",
    "    - require the output layer to have linear activation function\n",
    "2. **Huber Loss**\n",
    "    - $L=\\begin{cases} \n",
    "        \\frac{1}{2}(y-\\hat{y})^2 & \\text{for }|y-\\hat{y}|\\le \\delta\\\\\n",
    "        \\delta|y-\\hat{y}|-\\frac{1}{2}\\delta^2 & \\text{otherwise}\n",
    "        \\end{cases}$\n",
    "    - mse is not robust aganist outliers\n",
    "    - it combines both as mae and mse\n",
    "    - The delta parameter is the hyperparameter which can be adjustyed according as the requirment\n",
    "3. **Binary crossentropy**\n",
    "    - classification\n",
    "    - two classes\n",
    "    - $L=-ylog(\\hat{y})-(1-y)log(1-\\hat{y})$\n",
    "    - the output layer should have the activation function sigmoid.\n",
    "    - Adv \n",
    "        - Diffrentiable\n",
    "    - Dis adv\n",
    "        - not intuitave\n",
    "4. **Categorical cross entropy**\n",
    "    - used for multi class classification\n",
    "    - $L=-\\sum_{j=1}^K y_jlog(\\hat{y_j})$\n",
    "    - above is for single point, k is the number of classes\n",
    "    - activation function to be used is softmax\n",
    "    - $softmax=\\frac{e^{Z_1}}{e^{z_1}+e^{z_2}+e^{z_3}}$\n",
    "    - for another,$\\frac{e^{Z_2}}{e^{z_1}+e^{z_2}+e^{z_3}}$\n",
    "    - the sum will be 1 for all the above.\n",
    "5. **Sparse categorical crossentropy**\n",
    "    - we do the integer encoding of the data such as 1,2,3 \n",
    "    - The as in the above formula we do not find the $y_jlog(\\hat{y_j})$, we only find for what is the label\n",
    "    - if 1 then we multiply the corrspoding prbablity and then mutliply it with 1\n",
    "    - it has advantage that it is faster\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe63300",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
