{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5544691c",
   "metadata": {},
   "source": [
    "### **Adaptive Gradient**\n",
    "---------------------\n",
    "**Problems in the optimizers**\n",
    "- It perform better than other optimizers in where scale of input features is different.(although batch normalization is better in this case).\n",
    "- Elongated bowl problem(slope change in on axis and no change in the second axis): above optimizeres donot pefrom better\n",
    "- In case of the sparse features: due to the sparsity there would be a lot of values being zero as a result of which the wieghts donot get updated but bias get updated to much in case of batch gradient descnet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b213a3d0",
   "metadata": {},
   "source": [
    "- Adaptive gradient descent puts different learning rate for different parameters, thus we can optimize the learning rate( we can keep it small where derivative is large).\n",
    "\n",
    "**Update Rule:**\n",
    "\n",
    "- $W_{t+1}=W_t-\\eta \\frac{\\nabla W_t}{\\sqrt{V_t+\\epsilon}}$\n",
    "- $V_t=v_{t-1}+(\\nabla W_t)^2$\n",
    "- Here V is result of the sum of the past gradients, thus when past gradients are larger then learning rate will be divided by larger number as a result of which learning rate will be small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ffa816",
   "metadata": {},
   "source": [
    "**Disadvantages**\n",
    "- It can reach near to the solution but cannot reach to the minimum, since the learing rate decreases and updates will be too small."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
