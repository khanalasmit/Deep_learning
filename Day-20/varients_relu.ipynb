{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b332d4",
   "metadata": {},
   "source": [
    "### **Varients of the activation function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a95778",
   "metadata": {},
   "source": [
    "**Dying relu problem**\n",
    "- for whatever the input, some neurons give the zero output called dead neuron. If a neuron is dead once, it will always be dead\n",
    "- if more than half of the neuron are dead then complex patterns are not captured\n",
    "---------------------\n",
    "Consider a simple NN, with one input and one output layer with one neurons each.\n",
    "- Then, $a_1=max(0,z_1)$\n",
    "- $Z_1=w_1X_1+w_2x_2+b_1$\n",
    "- when the eqn above is less than zero then $a_1$ will be zero then the derivatives will be zero\n",
    "- The\n",
    "$$\n",
    "w_1=w_1-\\eta \\frac{\\partial L}{\\partial w_1}\\\\\n",
    "\\frac{\\partial L}{\\partial w_1}=\\frac{\\partial l}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial a_1}\\frac{\\partial a_1} {\\partial z_1}\\frac{\\partial z_1}{\\partial w_1}\n",
    "$$\n",
    ", here the derivative $\\frac{\\partial a_1}{\\partial z_1}$ is zero thus there is no update of the weight\n",
    "- The term $z_1$ will be zero when the learning rate is high causing the weights to be negetive leading to z in the next cycle negetive. The another reason is the high negetive bias.\n",
    "- The bias is negetive when the learning rate is too high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239c7703",
   "metadata": {},
   "source": [
    "**Solutions**\n",
    "- set low learning rate\n",
    "- set the bias to the postive valie(best value is 0.1)\n",
    "- use the varients of the relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fe6fd0",
   "metadata": {},
   "source": [
    "**Leaky relu**\n",
    "- $f(z)=max(0.01z,z)$\n",
    "- here when the value of the z is negetive then the least value will be 0.01\n",
    "- it is non saturating \n",
    "- easily computed\n",
    "- no dying relu problem\n",
    "- close to zero centered since both positive and the negetive value are there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffe5cd3",
   "metadata": {},
   "source": [
    "**Parametric relu**\n",
    "- $f(x)=\\begin{cases} \n",
    "x\\text{ if }x>0\\\\\n",
    "ax\\text{ otherwise } \\end{cases} $\n",
    "- here a is a trainable parameter\n",
    "- it is exactly the same to leaky relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50187e05",
   "metadata": {},
   "source": [
    "**Elu-exponential Linear unit**\n",
    "- $ELU(x)=\\begin{cases}\n",
    "x \\text{ if }x>0\\\\\n",
    "\\alpha (e^{x}-1)\\text{ if }x<0 \\end{cases}$\n",
    "- alpha is a changable paramters\n",
    "- it is a diffrentiable function\n",
    "- it is close to zero centered\n",
    "- better generalized\n",
    "- no dying relu problem\n",
    "--------------------\n",
    "**Disadvantages**\n",
    "- computationally expensive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a1dbea",
   "metadata": {},
   "source": [
    "**Selu**\n",
    "- $SELU(x)=\\lambda \\begin{cases}\n",
    "x \\text{ if }x>0\\\\\n",
    "\\alpha (e^{x}-1)\\text{ if }x\\le0 \\end{cases}$\n",
    "- it has the better results\n",
    "- it is recent\n",
    "- self normalization\n",
    "- the result are normalized thus faster convergence.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95591db",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
