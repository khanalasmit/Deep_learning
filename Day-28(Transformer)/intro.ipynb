{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba8b8a99",
   "metadata": {},
   "source": [
    "### **What is Tranformer**\n",
    "- It is a neural network architecture\n",
    "- They works very good for sequence to sequence task\n",
    "- Used in text summarization\n",
    "- Question answering system\n",
    "- we here use the mechanism called self attentation\n",
    "- we can give the text as a whole unlike the encoders and decoder.\n",
    "\n",
    "-----------\n",
    "\n",
    "**Impact of transformers**\n",
    "- Revolution in Natural Language processing\n",
    "- Democartizing AI\n",
    "    - Stable architecture\n",
    "    - The advanced architectures can be used for the specific small functions\n",
    "    - Here the transfer learning concept is brought to the AI\n",
    "- Multimodel capability\n",
    "    - flexible architecture\n",
    "- Accleration of GenAI\n",
    "- Unification of deep learning\n",
    "\n",
    "------------------------------\n",
    "\n",
    "**The origin story**\n",
    "- the sequence to sequence task can be handeled using encoder and decoder\n",
    "- The concept of attention\n",
    "    - decoder dynamically knows which context to give to the decoder\n",
    "    - here we cannot use the concept of transfer learning since the words are sent sequentially and then we need to train the model from scratch\n",
    "- The tranformer is the high level attentation mechanism called self attention \n",
    "    - We can train the architecture parallely\n",
    "    - then we can train on large dataset\n",
    "\n",
    "--------------\n",
    "\n",
    "**Diadvantages**\n",
    "- Requires the high computation\n",
    "- Data hungry\n",
    "- Overfitting\n",
    "- Energy consumption\n",
    "- Interpretibility\n",
    "- Bias "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9f67b2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
