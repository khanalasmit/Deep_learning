{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d12a82",
   "metadata": {},
   "source": [
    "### **GRADIENT DESCENT**\n",
    "1. Batch gradient descent(Vanilla gd)\n",
    "    - Has the weights updates as many times as epochs\n",
    "2. Stochastic gradient descent\n",
    "    - more frequent weight updates.\n",
    "    - it reduces the bias\n",
    "    - it more more operation costly for same number of epochs\n",
    "    - it converges faster than the batch gradient descent.\n",
    "    - since it requires less epochs thus we can say stochastic gradient descent in terms of convergence is faseter.\n",
    "    - the reduction of loss is smooth.\n",
    "    - The reson for the above point is that we take a point randomly and according to the point we update. Thus there is randomness in the reduction of the loss.\n",
    "    - This donot lead to the local minima\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cabe99a",
   "metadata": {},
   "source": [
    "**Vectorization**\n",
    "- using the dot product of finding the weights is called as vectorization.\n",
    "- This has the downside that we are doing dot product of the whole data loading the whole data into the RAM at once thus cannot be used for the very big data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0471312b",
   "metadata": {},
   "source": [
    "**Mini batch gradient descent**\n",
    "- we tak the batch of data in each epochs which is the intermediate line between the stochastic and the batch gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d61b7a4",
   "metadata": {},
   "source": [
    "**Why is the batch size in the multiple of 2?**\n",
    "- it is done for the effecitve use of the ram\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
